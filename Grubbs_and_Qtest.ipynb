{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some outlier detection techniques\n",
    "\n",
    "### Paul Anzel, 1/7/16\n",
    "\n",
    "### A bit added 1/10/16 when I finally looked through $P_n$ again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic technique I was taught for finding outliers was to find the $z$-score of various points--find the mean $\\mu$ and the standard deviation $\\sigma$ and calculate\n",
    "\n",
    "$$z_i = \\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "(and with various extensions for multidimensional data).\n",
    "\n",
    "This is fine as it goes for when you have hundreds of points, a tiny fraction of outliers, and your data is normally distributed. However, for the data I'm having to deal with, this doesn't actually work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean x = 14294.6\n",
      "StDev x = 37793.0\n",
      "[-0.378 -0.378 -0.378  2.268 -0.378 -0.378 -0.378]\n"
     ]
    }
   ],
   "source": [
    "# x[3] is a bogus datapoint where a scraper gave a dumb datapoint\n",
    "x = np.array([10, 11, 10, 100001, 9, 10, 11])\n",
    "mean_x = x.mean()\n",
    "print('Mean x = %.1f' % mean_x)\n",
    "std_x = x.std(ddof=1)\n",
    "print('StDev x = %.1f' % std_x)\n",
    "z_score = (x - mean_x)/std_x\n",
    "print(z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we had set some limit $z < 2.5$, we'd completely miss the bogus point. In fact, $z$-scores are not the best in general for small data sets, since the largest $z$ score you can get for $n$ data points is $(n-1)/\\sqrt{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try using a more robust estimate for the middle point with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median x = 10.0\n",
      "[ 0.     0.     0.     2.646 -0.     0.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "median_x = np.median(x)\n",
    "print('Median x = %.1f' % median_x)\n",
    "zm_score = (x - median_x)/std_x\n",
    "print(zm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that would still fail if we used $z < 3$ as a threshold. The next thing would then be to use a more robust form to estimate the variance (say the Interquartile Range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR = 1.0\n",
      "[      0.          1.349       0.     134887.859      -1.349       0.\n",
      "       1.349]\n"
     ]
    }
   ],
   "source": [
    "q75, q25 = np.percentile(x, [75, 25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.1f' % iqr)\n",
    "zmi_score = (x - median_x)/fake_std\n",
    "print(zmi_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can run into some issues, particularly if we have very few data-points (how do you even really define IQR on all of 3 data points?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR = 24998.75 :(\n",
      "[-0.    -0.     0.     5.396]\n",
      "IQR = 743.50 :(\n",
      "[   1.795   -0.003   -0.003   -0.001    0.001  181.422]\n"
     ]
    }
   ],
   "source": [
    "x2 = [9, 10, 11, 100001]\n",
    "q75, q25 = np.percentile(x2, [75 ,25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.2f :(' % iqr)\n",
    "zmi2 = (x2 - np.median(x2))/fake_std\n",
    "print(zmi2)\n",
    "\n",
    "x3 = [1000, 9, 9, 10, 11, 100001]\n",
    "q75, q25 = np.percentile(x3, [75, 25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.2f :(' % iqr)\n",
    "zmi3 = (x3 - np.median(x3))/fake_std\n",
    "print(zmi3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR = 496.50 :(\n",
      "[   2.69    -0.003   -0.003   -0.003    0.       0.003  271.677]\n"
     ]
    }
   ],
   "source": [
    "x4 = np.array([1000, 9, 9, 9, 10, 11, 100001])\n",
    "q75, q25 = np.percentile(x4, [75, 25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.2f :(' % iqr)\n",
    "zmi4 = (x4 - np.median(x4))/fake_std\n",
    "print(zmi4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potential robust dispersion metric to use is MAD, the Median Absolute Deviation.\n",
    "\n",
    "$$MAD = \\text{median} \\left| \\, \\bar x - \\text{median}(\\bar x) \\, \\right| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abs median difference\n",
      "[   990.      1.      1.      1.      0.      1.  99991.]\n",
      "MAD = 1.000\n"
     ]
    }
   ],
   "source": [
    "print('Abs median difference')\n",
    "print(np.abs(x4 - np.median(x4)))\n",
    "MAD = np.median(np.abs(x4 - np.median(x4)))\n",
    "print('MAD = %.3f' % MAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normal distributions, $MAD = \\sigma/1.4826$ and $IQR = 1.349\\sigma$. Note that both IQR and MAD assume the underlying distribution is not skewed, and may run into issues with multimodal data. It's also worth noting that we're trading efficency for robustness as we move away from the mean and standard deviation. But this is all in the domain of being approximately right than precisely wrong.\n",
    "\n",
    "For skewed data and as a more efficient estimator, we can look into $S_n$ and $Q_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rousseeuw's and Croux' [$S_n$ and $Q_n$](http://wis.kuleuven.be/stat/robust/papers/publications-1993/rousseeuwcroux-alternativestomedianad-jasa-1993.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_n = 1.1926 \\times \\text{median}_i (\\text{median}_j |x_i - x_j|)$$\n",
    "\n",
    "$$Q_n = 2.2219 \\times (k^{th} \\text{ order statistic of } |x_i - x_j| \\; i < j)$$\n",
    "\n",
    "$$k = \\text{h-choose-2} \\approx (\\text{n-choose-2})/4, \\; h = \\left \\lfloor{n/2}\\right \\rfloor + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_7 = 1.193\n"
     ]
    }
   ],
   "source": [
    "# Don't implement these in your code, there's a link to a more computationally efficent method below\n",
    "# plus this is not vectorized\n",
    "def find_Sn(x):\n",
    "    n = len(x)\n",
    "    outer_med_array = np.empty(n)\n",
    "    for ind in xrange(n):\n",
    "        outer_med_array[ind] = np.median(np.abs(x-x[ind]))\n",
    "    return 1.1926*np.median(outer_med_array)\n",
    "print('S_%d = %.3f' % (len(x4), find_Sn(x4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 6\n",
      "Sorted array:\n",
      "[     0.      0.      0.      1.      1.      1.      1.      2.      2.\n",
      "      2.    989.    990.    991.    991.    991.  99001.  99990.  99991.\n",
      "  99992.  99992.  99992.]\n",
      "Q_7 = 2.222\n"
     ]
    }
   ],
   "source": [
    "# Don't do this naive implementation either\n",
    "from math import floor\n",
    "from sklearn.cross_validation import LeavePOut\n",
    "def find_Qn(x):\n",
    "    n = len(x)\n",
    "    h = floor(n/2) + 1\n",
    "    k = int(h*(h-1)/2)\n",
    "    nchoose2 = int(n*(n-1)/2)\n",
    "    diff_array = np.empty(nchoose2)\n",
    "    # There's probably a better/faster way to get the n-choose-2 combinations of indicies, but I'm lazy\n",
    "    # and I know this works\n",
    "    lpo = LeavePOut(n, p=2)\n",
    "    for ind, (train, test) in enumerate(lpo):\n",
    "        diff_array[ind] = np.abs(x[test[1]] - x[test[0]])\n",
    "    diff_array.sort()\n",
    "    print('k = %d' % k)\n",
    "    print('Sorted array:')\n",
    "    print(diff_array)\n",
    "    return 2.2219*diff_array[k-1]  # Since Python zero-indexes, use k-1 for kth order stat\n",
    "print('Q_%d = %.3f' % (len(x4), find_Qn(x4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of scale estimators\n",
    "\n",
    "| Method | Efficency on Guassian | Breakdown point | Use on highly skewed data? |\n",
    "|:------:|:---------:|:-----------:|:----:|\n",
    "| Stdev    |  100%      | 0%         | ??? |\n",
    "| IQR    |  37%      | 25%         | No |\n",
    "| MAD    |  37%      | 50%         | No |\n",
    "| $S_n$  |  58%      | $\\left \\lfloor{n/2}\\right \\rfloor /n \\approx 50$%   | OK |\n",
    "| $Q_n$  |  82%      | $\\left \\lfloor{n/2}\\right \\rfloor /n \\approx 50$% | OK |\n",
    "| $P_n$  |  86%      | 13.4% | OK |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages we get for $S$ and $Q$ are balanced by longer computing time and space, but computers these days have plenty of memory and speed. R and C came up with a more efficent method to calculate these two scale statistics, details of which can be seen [at this link](http://feb.kuleuven.be/public/ndbae06/pdf-files/snqn.pdf). I do not know of any Python packages that implement these calculations, but the R package [robustbase](https://cran.r-project.org/web/packages/robustbase/index.html) does have them.\n",
    "\n",
    "As for which one to use, the paper [Study of Extreme Values and Influential Observations](http://www.amstat.org/sections/srms/proceedings/papers/2000_085.pdf) states that $S$ is a little less sensitive to errors (specifically gross-error sensitivity, see paper), and recommends using it for when $n < 50$. For larger datasets, use $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarr, Mueller, and Weber's [$P_n$](http://www.tandfonline.com/doi/abs/10.1080/10485252.2011.621424)\n",
    "\n",
    "Also see [this link](http://www.maths.usyd.edu.au/u/gartht/GarthTarrICORS.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, instead of taking n-choose-2 pairwise absolute differences as in $Q$, take n-choose-2 pairwise means and then find the IQR.\n",
    "\n",
    "$$P_n = 1.048 \\times IQR\\left[\\frac{x_i + x_j}{2} \\; i < j\\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_7 = 52395.284\n"
     ]
    }
   ],
   "source": [
    "def find_Pn(x):\n",
    "    n = len(x)\n",
    "    h = floor(n/2) + 1\n",
    "    k = int(h*(h-1)/2)\n",
    "    nchoose2 = int(n*(n-1)/2)\n",
    "    sum_array = np.empty(nchoose2)\n",
    "    # There's probably a better/faster way to get the n-choose-2 combinations of indicies, but I'm lazy\n",
    "    # and I know this works\n",
    "    lpo = LeavePOut(n, p=2)\n",
    "    for ind, (train, test) in enumerate(lpo):\n",
    "        sum_array[ind] = float(x[test[1]] + x[test[0]])/2\n",
    "    q75, q25 = np.percentile(sum_array, [75, 25])\n",
    "    return 1.048*(q75 - q25)\n",
    "print('P_%d = %.3f' % (len(x4), find_Pn(x4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks and we've exceeded the break-point for $P_n$ (29% of x4 is bad data). However, for a less fraught data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_14 = 1.310\n"
     ]
    }
   ],
   "source": [
    "x5 = np.array([10, 9, 9, 9, 10, 11, 100001, 11, 10, 12, 8, 10, 9, 11])\n",
    "print('P_%d = %.3f' % (len(x5), find_Pn(x5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more robust version of $P$, the authors provide $\\tilde{P}_n$ where you iteratively toss out anything with a z-like score > 5 (using $P_n$ as your scale parameter), reaching a 50% break point.\n",
    "\n",
    "More interestingly, $P_n$ performs more robustly with digitized data. For example, if you were drawing from $Poisson(1)$, it's highly likely that your $Q$ estimators will return 0 (and $S$ has troubles with $\\mu < 0.5$ here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_50 = 1.193\n",
      "k = 325\n",
      "Sorted array:\n",
      "[ 0.  0.  0. ...,  3.  3.  3.]\n",
      "Q_50 = 0.000\n",
      "P_50 = 1.048\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(200)\n",
    "poisson_test = sp.stats.poisson.rvs(1, size=50)\n",
    "print('S_%d = %.3f' % (len(poisson_test), find_Sn(poisson_test)))\n",
    "print('Q_%d = %.3f' % (len(poisson_test), find_Qn(poisson_test)))\n",
    "print('P_%d = %.3f' % (len(poisson_test), find_Pn(poisson_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a [clever algorithm](http://epubs.siam.org/doi/abs/10.1137/0207013) to estimate $P$ in $O(n \\log n)$ time, putting it on computing-time par with $S$ and $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hodges-Lehmann(-Sen)](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177704172) location estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, there's also a similar (and older) estimate for the position parameter (mean/median). The Hodges-Lehmann (or sometimes Hodges-Lehmann-Sen estimator) is given by\n",
    "\n",
    "$$HL = \\text{median} \\left[\\frac{x_i + x_j}{2} \\; i < j\\right]$$\n",
    "\n",
    "Following the same algorithm linked above, you can compute this in $O(n \\log n)$ time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HL of x_4 = 504.500. We're right at the breakpoint here.\n",
      "HL of x_5 = 10.000\n"
     ]
    }
   ],
   "source": [
    "# Naive approach, not the clever algorithm.\n",
    "def find_HL(x):\n",
    "    n = len(x)\n",
    "    h = floor(n/2) + 1\n",
    "    k = int(h*(h-1)/2)\n",
    "    nchoose2 = int(n*(n-1)/2)\n",
    "    sum_array = np.empty(nchoose2)\n",
    "    # There's probably a better/faster way to get the n-choose-2 combinations of indicies, but I'm lazy\n",
    "    # and I know this works\n",
    "    lpo = LeavePOut(n, p=2)\n",
    "    for ind, (train, test) in enumerate(lpo):\n",
    "        sum_array[ind] = float(x[test[1]] + x[test[0]])/2\n",
    "    return np.median(sum_array)\n",
    "print(\"HL of x_4 = %.3f. We're right at the breakpoint here.\" % (find_HL(x4)))\n",
    "print('HL of x_5 = %.3f' % (find_HL(x5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Location efficencies\n",
    "\n",
    "| Method | Efficency on Gaussian | Breakdown point |\n",
    "|:------:|:---------:|:-----------:|\n",
    "| Mean    |  100%      | 0%       |\n",
    "| Median    |  64%      | 50%       |\n",
    "| Hodges-Lehmann    |  86%     | 29% |\n",
    "\n",
    "Maybe at some point I'll look at the Huber estimator. https://projecteuclid.org/download/pdf_1/euclid.aoms/1177703732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's some other methods that are less common now but may pop up nonetheless..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dixon's Q test](https://en.wikipedia.org/wiki/Dixon%27s_Q_test)\n",
    "\n",
    "[Original paper](http://depa.fquim.unam.mx/amyd/archivero/ac1951_23_636_13353.pdf)\n",
    "\n",
    "[Python implementation](http://sebastianraschka.com/Articles/2014_dixon_test.html) - source for much of the code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is this--sort the data, and look at the minimum and maximum data points. For these points, calculate\n",
    "\n",
    "$$ Q = \\frac{\\text{gap to next point}}{\\text{range}}$$\n",
    "\n",
    "If Q is above some critical value (viz, there's a massive gap) then there's a good chance that the point is an outlier. The critical values of Q are given below, for 3 points, 4, points, ..., 28 points. 90, 95, and 99 represent confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q90 = [0.941, 0.765, 0.642, 0.56, 0.507, 0.468, 0.437,\n",
    "       0.412, 0.392, 0.376, 0.361, 0.349, 0.338, 0.329,\n",
    "       0.32, 0.313, 0.306, 0.3, 0.295, 0.29, 0.285, 0.281,\n",
    "       0.277, 0.273, 0.269, 0.266, 0.263, 0.26\n",
    "      ]\n",
    "\n",
    "q95 = [0.97, 0.829, 0.71, 0.625, 0.568, 0.526, 0.493, 0.466,\n",
    "       0.444, 0.426, 0.41, 0.396, 0.384, 0.374, 0.365, 0.356,\n",
    "       0.349, 0.342, 0.337, 0.331, 0.326, 0.321, 0.317, 0.312,\n",
    "       0.308, 0.305, 0.301, 0.29\n",
    "      ]\n",
    "\n",
    "q99 = [0.994, 0.926, 0.821, 0.74, 0.68, 0.634, 0.598, 0.568,\n",
    "       0.542, 0.522, 0.503, 0.488, 0.475, 0.463, 0.452, 0.442,\n",
    "       0.433, 0.425, 0.418, 0.411, 0.404, 0.399, 0.393, 0.388,\n",
    "       0.384, 0.38, 0.376, 0.372\n",
    "       ]\n",
    "\n",
    "Q90 = {n:q for n,q in zip(range(3,len(q90)+1), q90)}\n",
    "Q95 = {n:q for n,q in zip(range(3,len(q95)+1), q95)}\n",
    "Q99 = {n:q for n,q in zip(range(3,len(q99)+1), q99)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For x2 above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 11, 100001]\n"
     ]
    }
   ],
   "source": [
    "x2.sort()\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_min = 0.000\n",
      "Q_max = 1.000\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "Q_min = (x2[1]-x2[0])/(x2[-1]-x2[0])\n",
    "print('Q_min = %.3f' % Q_min)\n",
    "Q_max = (x2[-1]-x2[-2])/(x2[-1]-x2[0])\n",
    "print('Q_max = %.3f' % Q_max)\n",
    "print(Q_max > Q95[len(x2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to note:\n",
    "- This algorithm only works for one point at a time, though you could potentially iterate it (Risky! Think about [10, 10.1, 11, 1000] with Q90). There is an extension for multiple outliers, but it's not often used.\n",
    "- The algorithm assumes that the underlying data is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Grubbs' test](https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another outlier test. To do this, compute the $z$ scores of the sample, and find the largest absolute value, $G = \\max |z|$. We expect there's no outliers at significance level $\\alpha$ if\n",
    "\n",
    "$$ G > \\frac{N-1}{\\sqrt{N}} \\sqrt{\\frac{t^2_{\\alpha/2N, N-2}}{N - 2 + t^2_{\\alpha/2N, N-2}}}$$\n",
    "\n",
    "with $t_{\\alpha/2N, N-2}$ the upper critical value of the t-distribution with $N - 2$ degrees-of-freedom at significance level $\\alpha/2N$. For a 1-sided test, use $t_{\\alpha/N, N-2}$.\n",
    "\n",
    "Run this, removing the biggest outlier each time (if it exists), until it stops. But don't run this on too-small sample sizes (say $N \\leq 6$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def Grubbs_outlier_test(y_i, alpha=0.95):\n",
    "    \"\"\"\n",
    "    Perform Grubbs' outlier test.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    y_i (list or numpy array) - dataset\n",
    "    alpha (float) - significance cutoff for test\n",
    "\n",
    "    RETURNS\n",
    "    G_i (list) - Grubbs G statistic for each member of the dataset\n",
    "    Gtest (float) - rejection cutoff; hypothesis that no outliers exist if G_i.max() > Gtest\n",
    "    no_outliers (bool) - boolean indicating whether there are no outliers at specified\n",
    "    significance level\n",
    "    index (int) - integer index of outlier with maximum G_i\n",
    "    \n",
    "    Code from https://github.com/choderalab/cadd-grc-2013/blob/master/notebooks/Outliers.ipynb\n",
    "    \"\"\"\n",
    "    s = y_i.std()\n",
    "    G_i = np.abs(y_i - y_i.mean()) / s\n",
    "    N = y_i.size\n",
    "    t = stats.t.isf(1 - alpha/(2*N), N-2)\n",
    "    # Upper critical value of the t-distribution with N − 2 degrees of freedom and a\n",
    "    # significance level of α/(2N)\n",
    "    Gtest = (N-1)/np.sqrt(N) * np.sqrt(t**2 / (N-2+t**2))    \n",
    "    G = G_i.max()\n",
    "    index = G_i.argmax()\n",
    "    no_outliers = (G > Gtest)\n",
    "    return [G_i, Gtest, no_outliers, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.408  0.408  0.408  0.408  0.408  2.449  0.408]\n",
      "1.411\n",
      "True\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x_grubbs1 = np.array([9, 10, 11, 9, 10, 100000, 11])\n",
    "G1, Gtest, noout, indexval = Grubbs_outlier_test(x_grubbs1)\n",
    "print(G1)\n",
    "print(\"%.3f\" % Gtest)\n",
    "print(noout)\n",
    "print(indexval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of these tests assume that your data is normally distributed.\n",
    "- Let's say the data comes in digitized, as we get $[10, 12, 10]$. This might not raise any flags, but the Q-test would see a gap/range ratio of 1 for the 12 value, and flag it as an outlier. Alternately, if we had something like $[10, 11, 10, 11, 10, 11, 10, 11, 20]$ we'd not see the 20, despite looking pretty out of place.\n",
    "- If data is not normal, we can also run into problems--for example, if we had bimodal data centered around 1 and -1, it wouldn't be too unreasonable to draw $[1.01, -0.9, 1.13]$, but this would throw things into a loop. In this case, you just can't run these tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There's an extension of the Grubb's test called the [Tietjen-Moore test](http://www.itl.nist.gov/div898/handbook/eda/section3/eda35h2.htm), though this requires you to specify the number of outliers from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
