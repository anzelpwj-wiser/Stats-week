{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some outlier detection techniques\n",
    "\n",
    "### Paul Anzel, 12/16/15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic technique I was taught for finding outliers was to find the $z$-score of various points--find the mean $\\mu$ and the standard deviation $\\sigma$ and calculate\n",
    "\n",
    "$$z_i = \\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "(and with various extensions for multidimensional data).\n",
    "\n",
    "This is fine as it goes for when you have hundreds of points, a tiny fraction of outliers, and your data is normally distributed. However, for the data I'm having to deal with, this doesn't actually work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean x = 14294.6\n",
      "StDev x = 34989.5\n",
      "[-0.408 -0.408 -0.408  2.449 -0.408 -0.408 -0.408]\n"
     ]
    }
   ],
   "source": [
    "# x[3] is a bogus datapoint where a scraper gave a dumb datapoint\n",
    "x = np.array([10, 11, 10, 100001, 9, 10, 11])\n",
    "mean_x = x.mean()\n",
    "print('Mean x = %.1f' % mean_x)\n",
    "std_x = x.std()\n",
    "print('StDev x = %.1f' % std_x)\n",
    "z_score = (x - mean_x)/std_x\n",
    "print(z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we had set some limit $z < 2.5$, we'd completely miss the bogus point. We could try using a more robust estimate for the middle point with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median x = 10.0\n",
      "[ 0.     0.     0.     2.858 -0.     0.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "median_x = np.median(x)\n",
    "print('Median x = %.1f' % median_x)\n",
    "zm_score = (x - median_x)/std_x\n",
    "print(zm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that would still fail if we used $z < 3$ as a threshold. The next thing would then be to use a more robust form to estimate the variance (say the Interquartile Range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR = 1.0\n",
      "[      0.          1.349       0.     134887.859      -1.349       0.\n",
      "       1.349]\n"
     ]
    }
   ],
   "source": [
    "q75, q25 = np.percentile(x, [75 ,25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.1f' % iqr)\n",
    "zmi_score = (x - median_x)/fake_std\n",
    "print(zmi_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can run into some issues, particularly if we have very few data-points (how do you even really define IQR on all of 3 data points?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR = 24998.75 :(\n",
      "[-0.    -0.     0.     5.396]\n",
      "IQR = 743.50 :(\n",
      "[   1.795   -0.003   -0.003   -0.001    0.001  181.422]\n"
     ]
    }
   ],
   "source": [
    "x2 = [9, 10, 11, 100001]\n",
    "q75, q25 = np.percentile(x2, [75 ,25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.2f :(' % iqr)\n",
    "zmi2 = (x2 - np.median(x2))/fake_std\n",
    "print(zmi2)\n",
    "\n",
    "x3 = [1000, 9, 9, 10, 11, 100001]\n",
    "q75, q25 = np.percentile(x3, [75, 25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.2f :(' % iqr)\n",
    "zmi3 = (x3 - np.median(x3))/fake_std\n",
    "print(zmi3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR = 496.50 :(\n",
      "[   2.69    -0.003   -0.003   -0.003    0.       0.003  271.677]\n"
     ]
    }
   ],
   "source": [
    "x3 = [1000, 9, 9, 9, 10, 11, 100001]\n",
    "q75, q25 = np.percentile(x3, [75, 25])\n",
    "iqr = q75 - q25\n",
    "fake_std = iqr/1.349\n",
    "print('IQR = %.2f :(' % iqr)\n",
    "zmi3 = (x3 - np.median(x3))/fake_std\n",
    "print(zmi3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these really-low-quantity data sets, here's some alternate methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dixon's Q test](https://en.wikipedia.org/wiki/Dixon%27s_Q_test)\n",
    "\n",
    "[Original paper](http://depa.fquim.unam.mx/amyd/archivero/ac1951_23_636_13353.pdf)\n",
    "\n",
    "[Python implementation](http://sebastianraschka.com/Articles/2014_dixon_test.html) - source for much of the code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is this--sort the data, and look at the minimum and maximum data points. For these points, calculate\n",
    "\n",
    "$$ Q = \\frac{\\text{gap to next point}}{\\text{range}}$$\n",
    "\n",
    "If Q is above some critical value (viz, there's a massive gap) then there's a good chance that the point is an outlier. The critical values of Q are given below, for 3 points, 4, points, ..., 28 points. 90, 95, and 99 represent confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q90 = [0.941, 0.765, 0.642, 0.56, 0.507, 0.468, 0.437,\n",
    "       0.412, 0.392, 0.376, 0.361, 0.349, 0.338, 0.329,\n",
    "       0.32, 0.313, 0.306, 0.3, 0.295, 0.29, 0.285, 0.281,\n",
    "       0.277, 0.273, 0.269, 0.266, 0.263, 0.26\n",
    "      ]\n",
    "\n",
    "q95 = [0.97, 0.829, 0.71, 0.625, 0.568, 0.526, 0.493, 0.466,\n",
    "       0.444, 0.426, 0.41, 0.396, 0.384, 0.374, 0.365, 0.356,\n",
    "       0.349, 0.342, 0.337, 0.331, 0.326, 0.321, 0.317, 0.312,\n",
    "       0.308, 0.305, 0.301, 0.29\n",
    "      ]\n",
    "\n",
    "q99 = [0.994, 0.926, 0.821, 0.74, 0.68, 0.634, 0.598, 0.568,\n",
    "       0.542, 0.522, 0.503, 0.488, 0.475, 0.463, 0.452, 0.442,\n",
    "       0.433, 0.425, 0.418, 0.411, 0.404, 0.399, 0.393, 0.388,\n",
    "       0.384, 0.38, 0.376, 0.372\n",
    "       ]\n",
    "\n",
    "Q90 = {n:q for n,q in zip(range(3,len(q90)+1), q90)}\n",
    "Q95 = {n:q for n,q in zip(range(3,len(q95)+1), q95)}\n",
    "Q99 = {n:q for n,q in zip(range(3,len(q99)+1), q99)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For x2 above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 11, 100001]\n"
     ]
    }
   ],
   "source": [
    "x2.sort()\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_min = 0.000\n",
      "Q_max = 1.000\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "Q_min = (x2[1]-x2[0])/(x2[-1]-x2[0])\n",
    "print('Q_min = %.3f' % Q_min)\n",
    "Q_max = (x2[-1]-x2[-2])/(x2[-1]-x2[0])\n",
    "print('Q_max = %.3f' % Q_max)\n",
    "print(Q_max > Q95[len(x2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to note:\n",
    "- This algorithm only works for one point at a time, though you could potentially iterate it (Risky! Think about [10, 10.1, 11, 1000] with Q90). There is an extension for multiple outliers, but it's not often used.\n",
    "- The algorithm assumes that the underlying data is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Grubbs' test](https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another outlier test. To do this, compute the $z$ scores of the sample, and find the largest absolute value, $G = \\max |z|$. We expect there's no outliers at significance level $\\alpha$ if\n",
    "\n",
    "$$ G > \\frac{N-1}{\\sqrt{N}} \\sqrt{\\frac{t^2_{\\alpha/2N, N-2}}{N - 2 + t^2_{\\alpha/2N, N-2}}}$$\n",
    "\n",
    "with $t^2_{\\alpha/2N, N-2}$ the upper critical value of the t-distribution with N - 2 DOF at significance level $\\alpha/2N$.\n",
    "\n",
    "Run this, removing the biggest outlier each time (if it exists), until it stops. But don't run this on too-small sample sizes (say $N \\leq 6$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def Grubbs_outlier_test(y_i, alpha=0.95):\n",
    "    \"\"\"\n",
    "    Perform Grubbs' outlier test.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    y_i (list or numpy array) - dataset\n",
    "    alpha (float) - significance cutoff for test\n",
    "\n",
    "    RETURNS\n",
    "    G_i (list) - Grubbs G statistic for each member of the dataset\n",
    "    Gtest (float) - rejection cutoff; hypothesis that no outliers exist if G_i.max() > Gtest\n",
    "    no_outliers (bool) - boolean indicating whether there are no outliers at specified\n",
    "    significance level\n",
    "    index (int) - integer index of outlier with maximum G_i\n",
    "    \n",
    "    Code from https://github.com/choderalab/cadd-grc-2013/blob/master/notebooks/Outliers.ipynb\n",
    "    \"\"\"\n",
    "    s = y_i.std()\n",
    "    G_i = np.abs(y_i - y_i.mean()) / s\n",
    "    N = y_i.size\n",
    "    t = stats.t.isf(1 - alpha/(2*N), N-2)\n",
    "    # Upper critical value of the t-distribution with N − 2 degrees of freedom and a\n",
    "    # significance level of α/(2N)\n",
    "    Gtest = (N-1)/np.sqrt(N) * np.sqrt(t**2 / (N-2+t**2))    \n",
    "    G = G_i.max()\n",
    "    index = G_i.argmax()\n",
    "    no_outliers = (G > Gtest)\n",
    "    return [G_i, Gtest, no_outliers, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.408  0.408  0.408  0.408  0.408  2.449  0.408]\n",
      "1.411\n",
      "True\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x_grubbs1 = np.array([9, 10, 11, 9, 10, 100000, 11])\n",
    "G1, Gtest, noout, indexval = Grubbs_outlier_test(x_grubbs1)\n",
    "print(G1)\n",
    "print(\"%.3f\" % Gtest)\n",
    "print(noout)\n",
    "print(indexval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of these tests assume that your data is normally distributed.\n",
    "- Let's say the data comes in digitized, as we get $[10, 12, 10]$. This might not raise any flags, but the Q-test would see a gap/range ratio of 1 for the 12 value, and flag it as an outlier. Alternately, if we had something like $[10, 11, 10, 11, 10, 11, 10, 11, 20]$ we'd not see the 20, despite looking pretty out of place.\n",
    "- If data is not normal, we can also run into problems--for example, if we had bimodal data centered around 1 and -1, it wouldn't be too unreasonable to draw $[1.01, -0.9, 1.13]$, but this would throw things into a loop. In this case, you just can't run these tests.\n",
    "\n",
    "I'll need to look for additional routines for non-normal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
