{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Cross Validation\n",
    "\n",
    "### Paul Anzel, 10-20-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: I have a few different methods I'd like to try and do a fit (e.g. $k$-nearest-neighbors vs support-vector-machine) and I want to see which gives me better accuracy.\n",
    "\n",
    "Or...\n",
    "\n",
    "Problem: I don't have too much data, and I have a lot of possible parameters I could use for my statistical model fits. How can I choose among them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to try fitting to some data with an initial model\n",
    "\n",
    "$y = \\beta_1 x_2 + \\beta_2 x_3 + \\beta_3 x_7 + 5$\n",
    "\n",
    "$\\beta_2 = 5$\n",
    "\n",
    "$\\beta_3 = 10$ <-- the most important one\n",
    "\n",
    "$\\beta_7 = 0.2$\n",
    "\n",
    "But pretend I don't know about the values of the $\\beta$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "rd.seed(10000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 15\n",
    "\n",
    "beta2 = 5\n",
    "beta3 = 10\n",
    "beta7 = 2\n",
    "intercept = 50\n",
    "\n",
    "eta = 40  # Random parameter\n",
    "\n",
    "x1 = 10*(rd.rand(N) - .5)\n",
    "x2 = 10*(rd.rand(N) - .5)\n",
    "x3 = 10*(rd.rand(N) - .5)\n",
    "x4 = 10*(rd.rand(N) - .5)\n",
    "x5 = 10*(rd.rand(N) - .5)\n",
    "x6 = 10*(rd.rand(N) - .5)\n",
    "x7 = 10*(rd.rand(N) - .5)\n",
    "x8 = 10*(rd.rand(N) - .5)\n",
    "x9 = 10*(rd.rand(N) - .5)\n",
    "x10 = 10*(rd.rand(N) - .5)\n",
    "\n",
    "\n",
    "yvals = intercept + beta2*x2 + beta3*x3 + beta7*x7 + eta*rd.rand(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x109ecc250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anzelp/anaconda/lib/python2.7/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAERCAYAAAC5PCsTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103b486d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].scatter(x2, yvals, s=50, color='g')\n",
    "axes[0].set_xlabel('x2')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "axes[1].scatter(x3, yvals, s=50, color='g')\n",
    "axes[1].set_xlabel('x3')\n",
    "\n",
    "axes[2].scatter(x7, yvals, s=50, color='g')\n",
    "axes[2].set_xlabel('x7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7891585249\n",
      "[ -2.89937564   4.25268449  13.84027441   4.29652734   1.43866066\n",
      "   5.19036671  -1.59254485  -5.60143668  -4.34299036  -0.45154377]\n",
      "0.969906893742\n"
     ]
    }
   ],
   "source": [
    "x_collate = np.vstack((x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)).T\n",
    "\n",
    "clf_standard = linear_model.LinearRegression()\n",
    "clf_standard.fit(x_collate, yvals)\n",
    "print(clf_standard.intercept_)\n",
    "print(clf_standard.coef_)\n",
    "print(clf_standard.score(x_collate, yvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 only coef: 3.61\n",
      "x2 only coef: 4.44\n",
      "x3 only coef: 11.96\n",
      "x4 only coef: -1.81\n",
      "x5 only coef: -0.61\n",
      "x6 only coef: -0.69\n",
      "x7 only coef: 2.70\n",
      "x8 only coef: -1.22\n",
      "x9 only coef: -2.25\n",
      "x10 only coef: -3.65\n"
     ]
    }
   ],
   "source": [
    "for ind in range(10):\n",
    "    exec('xtest = x%d' % (ind + 1))\n",
    "    clf_simple = linear_model.LinearRegression()\n",
    "    clf_simple.fit(xtest.reshape((N,1)), yvals.reshape((N,1)))\n",
    "    print('x%d only coef: %.2f' % ((ind+1), clf_simple.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most conventional way of doing things. In holdout, the data is split into two sets, the training set and the test set (say 70%/30% split). We run our fit on the training set, and then see what error values you get when you apply the fit to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.495774445169\n",
      "-7.81171335028\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(\n",
    "    x_collate, yvals, test_size=3, random_state=0)\n",
    "\n",
    "clf_holdout=linear_model.LinearRegression().fit(X_train, Y_train)\n",
    "print('First try with a score R^2 = %.2f'\n",
    "      % clf_holdout.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(\n",
    "    x_collate, yvals, test_size=3, random_state=10)\n",
    "\n",
    "clf_holdout=linear_model.LinearRegression().fit(X_train, Y_train)\n",
    "print('Second time with a score R^2 = %.2f'\n",
    "      % clf_holdout.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sub-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do holdout $k$ times (selecting your sets randomly each time). After having done this, take the average of your errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 0  4  6 11  2 10  9 14  8 12 13  7] TEST: [5 3 1]\n",
      "TRAIN: [ 3  0 13 12  2  9 14  1  8  7 10  6] TEST: [11  4  5]\n",
      "TRAIN: [ 8  2  6  0  4 10  9 13 11  1  3  7] TEST: [14  5 12]\n",
      "TRAIN: [ 3  8  9 10 13  5  2 11  4  6 12  1] TEST: [ 7  0 14]\n",
      "TRAIN: [ 2  9 13  0  5 12  3  6  8 11  4 10] TEST: [14  7  1]\n",
      "TRAIN: [ 4  7 11  5 14 13  8  3  0  2 12  1] TEST: [ 9 10  6]\n",
      "TRAIN: [11  3  9  8  5  6  4  7 12  0  2 13] TEST: [ 1 14 10]\n",
      "TRAIN: [ 4 13  8  3  7 11 14  0 12  9  5  2] TEST: [ 6 10  1]\n",
      "TRAIN: [10  5  1  8 12  7 14  9  3 13 11  0] TEST: [2 4 6]\n",
      "TRAIN: [14  3 13  2  8 11  6  1  7  4  9 12] TEST: [ 5  0 10]\n",
      "-2.79454396274\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "\n",
    "ss = cross_validation.ShuffleSplit(\n",
    "    N, n_iter=n_iter, test_size=3, random_state=1337)\n",
    "\n",
    "R2_list = np.empty((len(ss),))\n",
    "ind = 0\n",
    "\n",
    "for train, test in ss:\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    X_train = x_collate[train, :]\n",
    "    X_test = x_collate[test, :]\n",
    "    Y_train = yvals[train]\n",
    "    Y_test = yvals[test]\n",
    "    clf=linear_model.LinearRegression().fit(X_train, Y_train)\n",
    "    R2_list[ind] = clf.score(X_test, Y_test)\n",
    "    ind += 1\n",
    "    \n",
    "print(R2_list.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV (Leave-one-out-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this technique, you'll take one data point out ($x_i$), and run your model fitting $\\hat y_{-i} = f(X_{-i})$ on the rest of the set. Calculate the error $e_{-i} = \\hat y_{-i} - y_i$. Next, repeat the process for each of the other data points $i = 1, ..., n$. Average the results, and you have your CV statistic.\n",
    "\n",
    "You can also average things like $R^2$ statistics instead (using scikit-learn's `clf.score()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loo = cross_validation.LeaveOneOut(N)\n",
    "\n",
    "for train, test in loo:\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    X_train = x_collate[train, :]\n",
    "    X_test = x_collate[test, :]\n",
    "    Y_train = yvals[train]\n",
    "    Y_test = yvals[test]\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a bit slow to do if you're implementing your own routine (SKLearn should have it optimized), but if you've got a linear regression system\n",
    "\n",
    "$y = X \\bar \\beta + \\bar e$\n",
    "\n",
    "You would (in a typical least-squares) compute\n",
    "\n",
    "$\\hat \\beta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "$\\hat y = X \\hat \\beta = X (X^T X)^{-1} X^T y = H y$\n",
    "\n",
    "And the cross-validation statistic can be computed\n",
    "\n",
    "$CV = \\frac{1}{n} \\sum_{i=0}^n [e_i / (1-H_{ii})]^2$\n",
    "\n",
    "Where $e_i$ are the residuals of the fit to all $n$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few additional notes:\n",
    "- LOOCV will not consistently (as $n \\rightarrow \\infty$) settle on a fit model ([Jun Shao, 1993](https://www.jstor.org/stable/2290328?&seq=1#page_scan_tab_contents)). This may not be a problem, as the real world barely follows nice models anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LpOCV (Leave-p-out-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extension of above, except rather than leaving 1 data point out and running $n$ different fits, you leave $p$ units out and do $\\frac{n!}{p! (n-p)!}$ (choosing all possible $p$-sized subsets) different statistical fits. Note that this can become fairly unweildly is $n$ is decently sized, so be warned.\n",
    "\n",
    "Like LOOCV, you'll run your fits on the n-choose-p take-p-out data sets, find the error on them, and average out all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 2\n",
    "\n",
    "lpo = cross_validation.LeaveOneOut(N, p=p)\n",
    "\n",
    "for train, test in lpo:\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    X_train = x_collate[train, :]\n",
    "    X_test = x_collate[test, :]\n",
    "    Y_train = yvals[train]\n",
    "    Y_test = yvals[test]\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different extension of LOOCV/LpOCV. Shuffle your data and divide it into $k$ subsets. Run your fitting $k$ times, where the $k^{th}$ subset is your test set and the rest are your training sets. Average your error among all of the different sets, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 5  6  7  8  9 10 11 12 13 14] TEST: [0 1 2 3 4]\n",
      "TRAIN: [ 0  1  2  3  4 10 11 12 13 14] TEST: [5 6 7 8 9]\n",
      "TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10 11 12 13 14]\n",
      "[-7.65582374  0.51350169 -1.46836761]\n"
     ]
    }
   ],
   "source": [
    "n_folds = 3\n",
    "\n",
    "kf = cross_validation.KFold(N, n_folds=n_folds)\n",
    "\n",
    "R2_list = np.empty((len(kf),))\n",
    "ind = 0\n",
    "\n",
    "for train, test in kf:\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    X_train = x_collate[train, :]\n",
    "    X_test = x_collate[test, :]\n",
    "    Y_train = yvals[train]\n",
    "    Y_test = yvals[test]\n",
    "    clf=linear_model.LinearRegression().fit(X_train, Y_train)\n",
    "    R2_list[ind] = clf.score(X_test, Y_test)\n",
    "    ind += 1\n",
    "    \n",
    "print(R2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trade off:\n",
    "\n",
    "- If you have a lot of fits (e.g. $k$ is pretty large in $k$-fold) the bias of your error estimator will be small (you'll be accurate) but it'll have large variance (low precision). This can also possibly take a long time to compute.\n",
    "- If you have a few fits (small $k$) the bias will be larger (higher than the true error rate) but will have less variance. Faster to compute.\n",
    "\n",
    "For large datasets, $k = 3$ is pretty reasonable. For very small ones, look to LOOCV. $k=10$ is a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn does this for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.65582374  0.51350169 -1.46836761]\n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation.cross_val_score(\n",
    "    linear_model.LinearRegression(), # Estimator\n",
    "    x_collate,\n",
    "    yvals,\n",
    "    cv=3 # Defaults to 3-Fold, can put in other generator (as above)\n",
    ")\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing time series, you want to test on the most recent data (so you're not just interpolating. Plus, it's clear that the data is not IID any longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technique:\n",
    "- Fit model to data $y_1, ..., y_t$. Find the forecast $\\hat y_{t+1}$ and compute prediction error $e_{t+1}$.\n",
    "- Repeat this for $t = m, ..., n-1$ where $m$ is the minimum number of steps needed to fit the model.\n",
    "- Average errors (as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other notes:\n",
    "- There are Stratification techniques, where you split your different categories (e.g. for classification) so that you get a (roughly) even partition of your different classes in your test groups (e.g. `cross_validation.StratifiedKFold`).\n",
    "- Similarly, for labeled categories you have methods like Leave-One-Label-Out and Leave-P-Labels-Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- http://robjhyndman.com/hyndsight/crossvalidation/\n",
    "- https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
